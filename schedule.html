<!DOCTYPE HTML>
<!--
	Solarize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Course Schedule</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body>

		<!-- Header Wrapper -->
			<div class="wrapper style1">

			<!-- Header -->
				<div id="header">
					<div class="container">

						<!-- Logo -->
							<h1><a href="#" id="logo">Big Data</a></h1>

						<!-- Nav -->
							<nav id="nav">
								<ul>
									<li class="active"><a href="index.html">Overview</a></li>
									<li><a href="syllabus.html">Syllabus</a></li>
									<li><a href="schedule.html">Course Schedule</a></li>
									<li><a href="project.html">Final Project</a></li>
									<li><a href="resources.html">Resources</a></li>
								</ul>
							</nav>

					</div>
				</div>
			</div>



		<!-- Main -->
			<div id="main" class="wrapper style6">

				<!-- Content -->
				<div id="content" class="container">
					<section>
						<header class="major">
							<h2 align="center">Mathematics of Big Data</h2>
							<span class="byline" align="center">
								Professor Weiqing Gu <br>
								Spring 2020
								
							</span>
						</header>

						<p>
							<b>Readings</b> should be done <i>before</i> class. All resources
							(including lecture slides, homework, starter files, hw solution,
							articles) can be found under the Resources tab.
						</p>

						<p>
							The topics to cover and the readings to be assigned are subject to change.
						</p>


						<table>
						    <thead>
						        <tr>
						            <th> <b>Date</b> </th>
						            <th> <b>Topics</b> </th>
						            <th> <b>Homework</b> </th>
						        </tr>
						    </thead>
						    <tbody>

						        <!-- Supervised Learning -->
						        <tr>
						            <td><b>Supervised Learning</b></br> Jan 27</td>
						            <td>
						                Introduction to Big Data <br>
														Linear Regression <br>
														Normal Equations and Optimization Techniques<br>
						                Linear Algebra Review<br>
														Covariance Matrix
						            </td>
						            <td>
						                <b>Read:</b> <br>
														Murphy 1.{all}</br>
														Murphy, 7.{1,...,5}
						            </td>
						        </tr>

						        <tr>
						            <td>Feb 3</td>
						            <td>
														Gaussian Distribution<br>
														Linear Regression (Probabilitic Approach) <br>
														Gradient Descent<br>
														Newton's Methods<br>
						                Logistic Regression<br>
														Exponential Family<br>
														Generalized Linear Models
						            <td>
													<b>Read:</b><br>
													Murphy, 8.{1,2,3,5} \ 8.{3.4,3.5}, <br>9.{1,2.2,2.4,3}
													<br><br>
													<b>Due:</b> <br>
														Homework 1<br>
								    						Brainstorm for midterm project
												</td>
						        </tr>

						        <tr>
						            <td>Feb 10</td>
						            <td>
														Probability Review<br>
						                Generalized Linear Models continued<br>
						                Poisson Regression<br>
														Softmax Regression<br>
						                Covariance matrix<br>
														Multivariate Gaussian Distribution<br>
														Marginalized Gaussian and the Schur Complement<br>
						            </td>
						            <td>
													<b>Read:</b> <br>
													Murphy 9.7, 4.{1,2,3,4,5,6} (important background)
													<br><br>
													<b>Due:</b> <br>
													Homework 2 <br>
								    					Project Proposal (<1 page)
												</td>
						        </tr>

						        <tr>
						            <td>Feb 17</td>
						            <td>
						                Dimensionality Reduction <br>
														Spectral Decomposition<br>
														Singular Value Decomposition<br>
						                Principal Component Analysis<br>
						                Generative Learning Algorithms<br>
						                Gaussian Discriminant Analysis<br>
														Cholesky Decomposition
						            </td>
						            <td>
													<b>Due:</b> <br>
													Final Project Proposal<br>
													Homework 3
						            </td>
						        </tr>

						        <tr>
						           	<
												<td>Feb 24</td>
						            <td>
						                Naive Bayes<br>
														L1 Regularization and Sparsity<br>
						                Lasso<br>
														Support Vector Machines<br>
														Kernels
						            </td>
						            <td>
													<b>Read:</b> <br>
													Murphy 14.{1,2,3,4} \ 14.{4.4}</br>
													<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">
														MapReduce: Simplified Data Processing on Large Clusters
													</a>
													</br><br>
													<b>Due:</b> <br>
													Homework 4
												</td>
						        </tr>

						        
						        <tr>
						        	<td><b> Unsupervised Learning </b></br>Mar 2</td>
						            <td>
						                Introduction to Unsupervised Learning<br>
						                Clustering<br>
														K-Means<br>
														Mixture of Gaussians<br>
														Jensen's inequality<br>
						                Expectation-Maximization (EM) Algorithm
						            </td>
						            <td>
													<b>Read:</b><br>
													Murphy 11.{1,2,3,4} \ 11.{4.6,4.9}</br>
													<a href="http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf">
														Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
													</a></br>
													<a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">
														Random Features for Large-Scale Kernel Machines</a>
													</br><br>
													<b>Due:</b> <br>
													Homework 5
						            </td>
							    </tr>
							<tr>
						            <td>Mar 9</td>
						            <td>
														Summary of EM Algorithm<br>
														EM for MAP estimation<br>
						                Kernel PCA<br>
														One Class Support Vector Machines<br>
														Learning Theory
						            </td>
						            <td>
													<b>Read:</b> <br>
													Murphy 12.2.{0,1,2,3} 14.4.4</br>
													<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.575&rep=rep1&type=pdf">
														Support Vector Method for Novelty Detection
													</a>
													<br></br>
													<b>Due:</b><br>
													Homework 6
						            </td>
						        </tr>
						        <tr>
								<td><b> Midterm Project Work </b></br>Mar 23</td>
						            <td>
						                Work on your midterm projects.<br>
						   
						            </td>
						            <td>
													<b>Read:</b><br>
													None</br>
													<b>Due:</b> <br>
													None
						            </td>
							    </tr>
						        <tr>
							<tr>
								<td><b> Midterm Project Presentation </b></br>Mar 30</td>
						            <td>
						                Be ready to present your midterm projects in class.<br>
						   
						            </td>
						            <td>
													<b>Read:</b><br>
													None</br>
													<b>Due:</b> <br>
													Midterm presentation and slides
						            </td>
							</tr>
							 <tr>
								<td><b> Midterm Project Due (11:59 pm) </b></br>Mar 31</td>
						            <td>
						                Your midterm projects must be sent to Prof. Gu via email by 11:59 pm.  
								    Your submission should include all relevant code and the .tex files for your essay.<br>
						   
						            </td>
						            <td>
													<b>Read:</b><br>
													None</br>
													<b>Due:</b> <br>
													Midterm project write-up.
						            </td>
							    </tr>
						        <tr>
						        

						        <tr>
						            <td><b> Learning Theory </b></br>Apr 6</td>
						            <td>
													Bayesian Learning<br>
													Bayesian Logistic and Linear Regressions (review)<br>
													Bayesian Inference<br>
													Intractable Integrals and Motivation for Approximate Methods<br>
													Learning Theory
						            </td>
						            <td>
													<b>Read:</b> <br>
													<a href="http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf">
														Large-Scale Sparse Principal Component Analysis with Application to Text Data
													</a></br>

													<a href="http://projecteuclid.org/download/pdf_1/euclid.aos/1176346060">
														On the Convergence Properties of the EM Algorithm
													</a>

													<br><br>
													<b>Due:</b> <br>
													Homework 7
												</td>
						        </tr>

						        <!-- Recommender Systems -->
						        <tr>
						            <td><b> Recommender Systems </b></br>Apr 13</td>
						            <td>
						                Introduction to Recommender Systems<br>
														Collaborative Filtering<br>
														Non-Negative Matrix Factorization<br>
						                Using Non-Negative Matrix Factorization for Topic
						                Modelling
						            </td>
						            <td>
					                <b>Read:</b><br>
													 Murphy 27.6.2</br>
					                <a href="http://sifter.org/~simon/journal/20061211.html">
														Netflix Update: Try This at Home</a>

													<br><br>
					                <b>Due:</b> <br>
													Homework 8<br>
													
						            </td>
						        </tr>

						        <!-- Graph Methods -->
						        <tr>
						            <td><b>Graph Methods</b></br>Apr 20</td>
						            <td>
						            	<!--
														Artificial Neural Network<br>
						                Graphs<br>
														Graph representations as data<br>
														The Laplacian
						                and usage of Spectral (Eigenvalue-Eigenvector) information
						                <br>
						                Directed Graphical Models (Bayesian Networks)<br>
														Conditional
						                Independence<br>
														Naive Bayes
						                as a Graphical Model<br>
														Plate Notation
										-->
														Additional topics will be covered in a workshop
														from 7:00 to 9:45 pm.
						            </td>
						            <td>
													<b>Read:</b> <br>
													Murphy 10.{1,2,3,4,5,6}
													<br><br>
													<b>Due:</b> <br>
													
													
						            </td>
						        </tr>
							<tr>
						            <td><b>Work on final</b></br>Apr 27</td>
						            <td>
						            	<!--
														Artificial Neural Network<br>
						                Graphs<br>
														Graph representations as data<br>
														The Laplacian
						                and usage of Spectral (Eigenvalue-Eigenvector) information
						                <br>
						                Directed Graphical Models (Bayesian Networks)<br>
														Conditional
						                Independence<br>
														Naive Bayes
						                as a Graphical Model<br>
														Plate Notation
										-->
														
						            </td>
						            <td>
													<b>Read:</b> <br>
													
													<br><br>
													<b>Due:</b> <br>
													
													
						            </td>
						        </tr>

						        <tr>
						            <td>May 4 or 11 (TBD)</td>
						            <td>
						                Final Project Presentation (Mon. 7-9:50 pm)
						            </td>
						            <td>
														<b>Due:</b><br>
														Final Project Presentation Slides
						            </td>
						        </tr>
							<tr>
						            <td>May 11</td>
						            <td>
						                Final Project Due for non (Tue. 11:59 pm)
						            </td>
						            <td>
														<b>Due:</b><br>
														Finish writing up final project
						            </td>
						        </tr>

										



						    </tbody>
						</table>

					</section>
				</div>
			</div>


			<!-- Copyright -->
				<div id="copyright">
					@Copyright 2020 Mathematics of Big Data Spring 2020 All Rights Reserved
				</div>

		</div>

	</body>
</html>
